# agent

An example program-under-observation instrumented with the OpenTelemetry Java agent.


## Overview

The tech stack in this subproject:

* A program-under-observation
  * This is a fictional "data processing" program written in Java. This program is instrumented with the OpenTelemetry
    Java agent.
* A metrics sink/collector (Telegraf)
  * Telegraf acts as a sink for the metrics pushed by the OpenTelemetry agent. Telegraf re-formats the metrics into an
    acceptable format for the metrics database and then writes the metrics into the database. Telegraf is acting as
    "collector" in the OpenTelemetry terminology (I think).
* A metrics database (InfluxDB)
  * InfluxDB is an open source time series database that's usually used for metrics. Prometheus is an even more popular
    alternative. There are many vendor options, too, like Datadog.

OpenTelemetry defines a protocol and conventions, and as such it comes with a lot of client libraries that implement the
protocol and conventions for metric-creation and metric-collection, but OpenTelemetry doesn't replace the database or
visualization tool. Remember, OpenTelemetry is not a complete observability stack. 

While OpenTelemetry operates in the realm of metrics, logs, and spans, I'm going to only implement a metrics example.


## Instructions

Follow these instructions to build and run the example system.

1. Pre-requisites: Java and Docker
    * I used Java 17.
2. Start infrastructure services
    * ```shell
      docker-compose up
      ```
    * This starts Telegraf and InfluxDB.
    * Pay attention to the output of these containers as they run. It's a tricky system to set up, and you'll want to
      know if there are any errors, like if Telegraf is unable to connect to InfluxDB.
3. Download the OpenTelemetry Java agent
    * ```shell
      AGENT_URL="https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/download/v1.32.0/opentelemetry-javaagent.jar"
      curl  --location --output opentelemetry-javaagent.jar "$AGENT_URL"
      ```
    * It's important that you use the `--location` (`-L`) flag because the GitHub URL redirects to some CDN URL at `https://objects.githubusercontent.com/...`.
    * Note: in a production codebase, it would be better to handle agent-related things (URL config, downloading the
      agent, Java options) in the Gradle build. Unfortunately, the Gradle code required (e.g. <https://stackoverflow.com/a/20968466>)
      is a bit cryptic and distracting, so it's better to do these steps manually for the sake of clarity and "learning
      the core concepts" instead of learning Gradle.
4. Build the program distribution
    * ```shell
      ./gradlew installDist
      ```
    * The distribution is in `build/install/agent/`. Notice the "start script" file at `bin/agent`.
      This script is generated by Gradle's built-in `application` plugin and the script provides extension points for us
      to some behavior. In particular, we'll use the environment variable `JAVA_OPTS` to set the `-javaagent` JVM option
      to instrument our program with the OpenTelemetry Java agent.
5. Run the program with the agent
    * ```shell
      JAVA_OPTS="-javaagent:$(pwd)/opentelemetry-javaagent.jar -Dotel.javaagent.configuration-file=$(pwd)/open-telemetry.properties" ./build/install/agent/bin/agent
      ```
    * The program will run indefinitely and continuously submit OTLP-based metrics data to the Telegraf server.
6. Wait two minutes
    * This is important! There is a natural delay throughout the system. The OpenTelemetry agent only collects
      new values at some interval (I can't find docs on this), and it only submits the data to Telegraf at some interval
      (I can't find docs on this). Telegraf only submits data to InfluxDB at some interval. We have to wait until the
      first batch of data is submitted to InfluxDB before we can inspect it.
7. Inspect the metrics in InfluxDB directly
    * Start an `influx` session inside the InfluxDB container with the following command.
    * ```shell
      docker exec -it agent-influxdb-1 influx -precision rfc3339
      ```
    * The `influx` session may remind you of a SQL sessions. In it, you can run commands like `SHOW DATABASES` and
      `SHOW MEASUREMENTS` to explore the data. We named our database `playground`. You should be able to connect to it
      by issuing a `use playground` command. Then, execute a `show measurements` command, and hopefully it shows the
      following metrics that have flowed from our program through Telegraf and into the Influx database. It should look
      something like the following.
    * ```text
      $ docker exec -it agent-influxdb-1 influx
      Connected to http://localhost:8086 version 1.8.10
      InfluxDB shell version: 1.8.10
      > use playground
      Using database playground
      > show measurements
      name: measurements
      name
      ----
      process.runtime.jvm.buffer.count
      process.runtime.jvm.buffer.limit
      process.runtime.jvm.buffer.usage
      process.runtime.jvm.classes.current_loaded
      process.runtime.jvm.classes.loaded
      process.runtime.jvm.classes.unloaded
      process.runtime.jvm.cpu.utilization
      process.runtime.jvm.gc.duration
      process.runtime.jvm.memory.committed
      process.runtime.jvm.memory.init
      process.runtime.jvm.memory.limit
      process.runtime.jvm.memory.usage
      process.runtime.jvm.memory.usage_after_last_gc
      process.runtime.jvm.system.cpu.load_1m
      process.runtime.jvm.system.cpu.utilization
      process.runtime.jvm.threads.count
      queueSize
      ```
    * Let's inspect the memory usage over time for ouro "data processing" program. This is captured in the `process.runtime.jvm.memory.usage`
      metric. Look at the below snippet for an example. The output shows the memory usage in MiB over time. The MiB over
      time is a bit erratic compared to a smooth sawtooth pattern because the metrics are only represented every minute,
      so we're left with a low resolution view of the data. But, it's still illuminating. The memory usage varies between
      40Mib and 166MiB in the ten-minute window shown below. 
    * ```text
      > SELECT SUM(gauge) / 1024 / 1024 AS "MiB" FROM "process.runtime.jvm.memory.usage" WHERE type = 'heap' GROUP BY time(1m)
      name: process.runtime.jvm.memory.usage
      time                 MiB
      ----                 ---
      2023-11-19T21:16:00Z 114.48226928710938
      2023-11-19T21:17:00Z 166.48226928710938
      2023-11-19T21:18:00Z 44.17877197265625
      2023-11-19T21:19:00Z 84.17877197265625
      2023-11-19T21:20:00Z 124.17877197265625
      2023-11-19T21:21:00Z 160.17877197265625
      2023-11-19T21:22:00Z 40.520263671875
      2023-11-19T21:23:00Z 80.520263671875
      2023-11-19T21:24:00Z 116.520263671875
      2023-11-19T21:25:00Z 156.520263671875
      2023-11-19T21:26:00Z
      ```
8. Stop the Java program
    * Press `Ctrl+C` to stop the program from the same terminal window where you ran the program.
9. Stop the infrastructure services
    * ```shell
      docker-compose down
      ```
    * I think it's important to do a proper `down` command so that the network is cleaned up. Otherwise, you might
      experience some weirdness if you change the Docker Compose file and then try to bring the services back up. Not
      really sure.


## Wish List

General clean-ups, TODOs and things I wish to implement for this project:

* [x] DONE Scaffold
* [x] DONE Do some "hello world"-style task on an indefinite loop. We must use a non-daemon thread. Our goal is to observe
  memory and garbage collection metrics which are affected by this task.
* [x] DONE Download and wire in the OpenTelemetry Java agent
* [x] DONE Debug logs for the OpenTelemetry Java agent. I'm not sure if it's working.
* [x] DONE Use a properties file. The commandline options are getting too long. With a properties file, you can lay out
  properties neatly on individual lines, and you can use comments.
* [x] DONE Set up Telegraf and InfluxDB using Docker Compose
   * DONE Step down to Influx v1. v2 is trouble because of Flux.
* [ ] Consider upgrading from the "hello world"-style task to a more realistic task. Use some framework/library that is instrumented
  by the OpenTelemetry Java agent. Maybe as simple as a cron job scheduled with Quartz? My goal is to exercise the
  instrumentation for a third-party library and see what the quality of metrics is like (naming, volume, etc. I'm not
  even sure what I'm looking for exactly, still learning the "what" of OpenTelemetry).
* [ ] Actually the agent is distributed in Maven Central? See [this example project in `open-telemetry/opentelemetry-java-examples`](https://github.com/open-telemetry/opentelemetry-java-examples/blob/5163caeca41033a910323e5de9efb76e4e775348/javaagent/build.gradle#L27).
* [x] DONE Forget Grafana? It's enough to just use the InfluxDB CLI to inspect the data. This is a more direct demo. 
* [x] DONE Slow down the simulated data processing. I want a smoother memory usage line.


## Reference

* [OpenTelemetry docs: *Automatic Instrumentation*](https://opentelemetry.io/docs/instrumentation/java/automatic/)
  * This is what I'm using in this project.
* [OpenTelemetry docs: *Manual Instrumentation* for Java](https://opentelemetry.io/docs/instrumentation/java/manual/)
  > Manual instrumentation is the act of adding observability code to an app yourself.
* [OpenTelemetry: *Semantic Conventions*](https://github.com/open-telemetry/semantic-conventions/blob/main/docs/README.md)
  > The benefit to using Semantic Conventions is in following a common naming scheme that can be standardized across a codebase, libraries, and platforms.
  * This is, to me, the strongest selling point in OpenTelemetry. Yet another specification can turn into "yet another
    abandoned specification on an ever-accumulating pile of noise". But, the sheer weight of OpenTelemetry and its
    adoption across vendors, libraries, marketing, and mind-share means that this "specification of conventions" has
    staying power. Good!
* [GitHub repo: `influxdata/influxdb-observability`](https://github.com/influxdata/influxdb-observability)
  > This repository is a reference for converting observability signals (traces, metrics, logs) to/from a common InfluxDB schema.
